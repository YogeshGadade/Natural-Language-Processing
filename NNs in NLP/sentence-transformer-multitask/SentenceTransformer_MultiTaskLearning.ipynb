{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/HUJCn8bnGW/cotKSsZvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YogeshGadade/Natural-Language-Processing/blob/main/SentenceTransformer_MultiTaskLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: Implement a Sentence Transformer Model\n"
      ],
      "metadata": {
        "id": "qQ59RNXhr7Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Set up environment\n",
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "7tKwWOKJsDVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Choose a Pre-Trained Transformer Model:\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "#For a sentence transformer, one can use a pre-trained model like bert-base-uncased or distilbert-base-uncased\n",
        "\n",
        "\n",
        "# 3. Tokenize Sentences: Convert sentences into tokens IDs that the transformer can understand\n",
        "# Sample sentences\n",
        "sentences = [\"This is a sample sentence.\", \"Transformers are great for NLP tasks.\", \"Embeddings capture the meaning of sentences.\"]\n",
        "# Extract the pooled output (sentence-level embeddings)\n",
        "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# 4. Pass the Tokenized sentences throught the transformer Model: Using the transformer model to get the the sentence embeddings (typically by using the output from the last hidden layer or a pooled output)\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "# Extract the pooled output (sentence-level embeddings): using the pooled output as the sentence embedding\n",
        "embeddings = outputs.pooler_output\n",
        "print(embeddings.shape) #  # This should print the shape of the embeddings tensor\n",
        "print(\"Embeddings:\", embeddings)"
      ],
      "metadata": {
        "id": "9FrH0Ee5XYxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output shows that for three sentences, the model has produced embeddings of size 768 for each sentence.\n",
        "\n",
        "*  **Sample Sentences**: I am using three sample sentences of varying lengths.\n",
        "*   **Tokenizer**: The BertTokenizer converts sentences into token IDs, with padding and truncation enabled to ensure all inputs are the same length.\n",
        "*   **Model**: The BertModel processes the tokenized sentences and outputs hidden states. I am using the pooler_output, which is a fixed-length vector representing the entire sentence.\n",
        "*   **Output**: The pooler_output provides sentence-level embeddings, which are printed along with their shape. This shape will be (batch_size, hidden_size) (e.g., (3, 768) for three sentences with a 768-dimensional embedding).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oyltGEdX9CZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture Choices\n",
        "1. **Why I Chose This Transformer Model (BERT):**\n",
        "  *   **BERT (Bidirectional Encoder Representations from Transformers)** is a widely-used transformer model that has been pre-trained on large amounts of text data. It is particularly good at understanding the context of words by reading the entire sentence in both directions (hence, bidirectional).\n",
        "  *   The bert-base-uncased model has 12 layers and 768-dimensional hidden representations, making it a strong choice for generating high-quality sentence embeddings.\n",
        "\n",
        "2. **Handling Input Sizes:**\n",
        "  *   **Padding and Truncation:** Since input sentences have different lengths, I used the padding=True option to ensure that all input sequences are the same length. The truncation=True option makes sure that if a sentence is too long, it is truncated to fit the model's input requirements.\n",
        "  *   Fixed-Length Embeddings: BERT outputs a fixed-length vector for each sentence regardless of the input sentence length, which is crucial for using these embeddings in downstream tasks (e.g., sentence classification).\n",
        "3. **Choice of Sentence Embeddings (Pooled Output):**\n",
        "  *   **Pooled Output:** I used the pooler_output from the BERT model as the sentence embedding. This is a 768-dimensional vector that represents the entire sentence. The pooler_output is taken from the first token (CLS token) and passed through a dense layer followed by a tanh activation function. This makes it ideal for sentence-level tasks like classification.\n",
        "  *   **Alternative: Average of Token Embeddings:** Another option would be to average the token embeddings across all positions, but the pooler_output is more efficient and is commonly used in sentence classification tasks.\n",
        "4. **Why Not Train From Scratch:**\n",
        "  *   Since BERT is pre-trained on a massive amount of text, it's highly effective for many downstream tasks without needing to train from scratch. The pre-trained embeddings already capture a lot of semantic information."
      ],
      "metadata": {
        "id": "1Ujr7A4B_p9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Transformer Model:** BERT (bert-base-uncased) was chosen for its robust pre-trained embeddings.\n",
        "2. **Handling Inputs:** I ensured all inputs were tokenized, padded, and truncated as needed.\n",
        "3. **Sentence Embeddings:** I chose the pooler_output from BERT as the fixed-length representation of sentences, which is ideal for tasks like sentence classification or semantic similarity.\n",
        "4. **Model Efficiency:** a pre-trained model like BERT allows us to get powerful sentence representations without the need to train a model from scratch."
      ],
      "metadata": {
        "id": "CBWMy4eqA90I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: Multi-Task Learning Expansion\n",
        "1.   **Add Task-Specific Heads:**\n",
        "  *   Multi-task learning requires the model to handle two tasks simultaneously, which means need to add separate heads (final layers) for each task.\n",
        "2. **Task A: Sentence Classification Head:**\n",
        "  *   This head will classify sentences into predefined categories (e.g., positive/negative sentiment or different topics).\n",
        "  *   I can add a linear layer after the sentence embeddings."
      ],
      "metadata": {
        "id": "0KNnEIdzBu4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, transformer, num_classes):\n",
        "        super(SentenceClassifier, self).__init__()\n",
        "        self.transformer = transformer\n",
        "        self.classifier = nn.Linear(transformer.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.pooler_output\n",
        "        logits = self.classifier(embeddings)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "in2T8vRZBkEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   **Task B: Additional NLP Task (Named Entity Recognition or Sentiment Analysis):**\n",
        "  *   Create a second head for another task. For example, if you choose Named Entity Recognition (NER), you’ll need a head that can classify each token.\n",
        "  *   Alternatively, you can add a head for sentiment analysis."
      ],
      "metadata": {
        "id": "CXu19VpRCYzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, transformer, num_classes_a, num_classes_b):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.transformer = transformer\n",
        "        self.classifier_a = nn.Linear(transformer.config.hidden_size, num_classes_a)\n",
        "        self.classifier_b = nn.Linear(transformer.config.hidden_size, num_classes_b)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.pooler_output\n",
        "        logits_a = self.classifier_a(embeddings)  # Task A (Sentence Classification)\n",
        "        logits_b = self.classifier_b(embeddings)  # Task B (e.g., NER or Sentiment Analysis)\n",
        "        return logits_a, logits_b\n"
      ],
      "metadata": {
        "id": "VW12b55dCn-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture Changes Summary\n",
        "The multi-task architecture was created by adding multiple task-specific heads on top of a shared transformer backbone. The transformer (e.g., BERT) encodes sentences into rich contextual embeddings, which are shared across tasks.\n",
        "\n",
        "  *  **Task A (Sentence Classification)** uses a simple linear layer that takes the sentence embeddings and outputs class logits, ideal for classification tasks given the richness of the embeddings.\n",
        "  *   **Task B (e.g., NER or Sentiment Analysis)** also uses the shared embeddings but has a separate head tailored to its specific output format (e.g., token-level predictions for NER or sentence-level for sentiment).\n",
        "\n",
        "**Why This Architecture?**\n",
        "  Sharing the transformer backbone across tasks ensures parameter efficiency and enables transfer learning, where knowledge from one task benefits the other. Task-specific heads allow the model to be flexible, providing the appropriate output for each task while leveraging a shared linguistic understanding from the transformer.\n",
        "\n",
        "This approach provides a balance of efficiency and task-specific customization, making it ideal for multi-task learning with related NLP tasks."
      ],
      "metadata": {
        "id": "suPigywPLZU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: Discussion Questions\n",
        "In a multi-task learning setup, there are various scenarios where freezing parts of the model makes sense:\n",
        "\n",
        "1. **Training Decisions (Freezing Parts of the Model)**\n",
        "- **Freeze transformer backbone**\n",
        "  - Freezing the backbone (pre-trained transformer) can be a good strategy when the model is already using a robust, pre-trained embedding, and the goal is to fine-tune only the task-specific heads. This helps avoid overfitting on smaller datasets and reduces training time. For example, if the transformer has been pre-trained on a massive dataset like BERT, it's beneficial to keep those weights fixed, as they are already well-optimized for general language understanding tasks.\n",
        "- **Train only task-specific heads**\n",
        "  - In some cases, one of the task-specific heads may perform well with little training, while the other task head needs further optimization. For instance, if Task A (sentence classification) shows satisfactory performance, I might freeze its head and focus on improving Task B (such as NER or sentiment analysis). This prevents unnecessary updates to an already well-performing part of the model while dedicating resources to the underperforming task.\n",
        "\n",
        "By making smart choices about which parts of the model to freeze, it is possible to effectively balance efficiency and performance during training.\n",
        "\n",
        "2. **Multi-Task Model vs. Separate Models**\n",
        "- **When to Use a Multi-Task Model:**\n",
        "  - A multi-task model is ideal when tasks are related and share common knowledge. For example, tasks like sentence classification and sentiment analysis are both based on understanding the semantics of the sentence, so they can benefit from the shared transformer backbone. Using a single model helps leverage common patterns learned from both tasks, resulting in better generalization and reduced computational resources compared to training separate models for each task.\n",
        "- **When to Use Separate Models:**\n",
        "  - Separate models should be considered when tasks are either too different or may interfere with each other during training. If Task A (sentence classification) and Task B (NER) are significantly different in their objectives, they might require specialized features that conflict with each other, leading to suboptimal performance in a multi-task setup. Separate models allow each task to have a dedicated architecture that is tailored to its specific needs.\n",
        "\n",
        "3. **Handling Imbalanced Data**\n",
        "When dealing with data imbalances between tasks (e.g., Task A has a lot of data and Task B has limited data), several strategies can be used:\n",
        "\n",
        "- **Up-sample/down-sample**\n",
        "  - One way to handle this imbalance is to up-sample the data for Task B by duplicating or augmenting its data points, or down-sample the data for Task A to ensure that both tasks have similar training sizes. This can help prevent the model from becoming biased towards Task A.\n",
        "\n",
        "- **Weighted Loss Functions**\n",
        "  - Another approach is to use a weighted loss function, where more importance is given to Task B during training. This ensures that Task B's performance is not neglected just because there is less data available for it. In this case, I would assign higher loss weights to Task B, allowing the model to prioritize learning from this scarce data.\n",
        "- **Transfer learning:**\n",
        "  - A good strategy would be to first train the model on the abundant Task A data, allowing the model to learn useful features. Then, I would fine-tune the model on Task B, leveraging the pre-trained knowledge from Task A to improve Task B’s performance despite having limited data. This transfer learning approach can significantly improve the model's performance on Task B."
      ],
      "metadata": {
        "id": "7n44uByofgNz"
      }
    }
  ]
}